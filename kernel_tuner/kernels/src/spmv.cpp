extern "C" int spmv(int nrow, const int* col, const double* val, const double* x, double* tmp, double* y) { 
    int i;

    asm volatile(
    " kxnorw %%k0, %%k0, %%k1                          \t\n"
    " kxnorw %%k0, %%k0, %%k2                          \t\n"
    " kxnorw %%k0, %%k0, %%k3                          \t\n"
    " kxnorw %%k0, %%k0, %%k4                          \t\n"
    " vmovdqa (%[COL]), %%ymm0                         \t\n"
    " vmovdqa 0x20(%[COL]), %%ymm1                     \t\n"
    " vmovdqa 0x40(%[COL]), %%ymm2                     \t\n"
    " vmovdqa 0x60(%[COL]), %%ymm3                     \t\n"
    " vgatherdpd (%[X],%%ymm0,8), %%zmm16%{%%k1%}      \t\n"
    " vgatherdpd (%[X],%%ymm1,8), %%zmm17%{%%k2%}      \t\n"
    " vgatherdpd (%[X],%%ymm2,8), %%zmm18%{%%k3%}      \t\n"
    " vgatherdpd (%[X],%%ymm3,8), %%zmm19%{%%k4%}      \t\n"
    " vmovdqa 0x80(%[COL]), %%ymm0                     \t\n"
    " vmovdqa 0xa0(%[COL]), %%ymm1                     \t\n"
    " vmovdqa 0xc0(%[COL]), %%ymm2                     \t\n"
    " vmovdqa 0xe0(%[COL]), %%ymm3                     \t\n"
    " vmovupd (%[VAL]), %%zmm24                        \t\n"
    " vmovupd 0x40(%[VAL]), %%zmm25                    \t\n"
    " vmovupd 0x80(%[VAL]), %%zmm26                    \t\n"
    " vmovupd 0xc0(%[VAL]), %%zmm27                    \t\n"
    " add $0x100, %[COL]                               \t\n"
    " add $0x100, %[VAL]                               \t\n"

    " sar $0x3, %[NROW]                                \t\n"
    " movl $0x8, %[I]                                  \t\n"
    " loop_spmv:                                       \t\n"

    " vmovupd %%zmm9, %%zmm8                           \t\n"
    " vmovupd %%zmm10, %%zmm9                          \t\n"
    " vmovupd %%zmm11, %%zmm10                         \t\n"
    " vmovupd %%zmm12, %%zmm11                         \t\n"
    " vmovupd %%zmm13, %%zmm12                         \t\n"
    " vmovupd %%zmm14, %%zmm13                         \t\n"
    " vmovupd %%zmm15, %%zmm14                         \t\n"
    " vmovupd (%[VAL]), %%zmm28                        \t\n"
    " vmovupd 0x40(%[VAL]), %%zmm29                    \t\n"
    " vmovupd 0x80(%[VAL]), %%zmm30                    \t\n"
    " vmovupd 0xc0(%[VAL]), %%zmm31                    \t\n"
    " kxnorw %%k0, %%k0, %%k1                          \t\n"
    " kxnorw %%k0, %%k0, %%k2                          \t\n"
    " kxnorw %%k0, %%k0, %%k3                          \t\n"
    " kxnorw %%k0, %%k0, %%k4                          \t\n"
    " prefetcht2 0x1200(%[COL])                        \t\n"
    " prefetcht2 0x1240(%[COL])                        \t\n"
    " vmovdqa (%[COL]), %%ymm4                         \t\n"
    " vmovdqa 0x20(%[COL]), %%ymm5                     \t\n"
    " vmovdqa 0x40(%[COL]), %%ymm6                     \t\n"
    " vmovdqa 0x60(%[COL]), %%ymm7                     \t\n"
    " vgatherdpd (%[X],%%ymm0,8), %%zmm20%{%%k1%}      \t\n"
    " vgatherdpd (%[X],%%ymm1,8), %%zmm21%{%%k2%}      \t\n"
    " vgatherdpd (%[X],%%ymm2,8), %%zmm22%{%%k3%}      \t\n"
    " vgatherdpd (%[X],%%ymm3,8), %%zmm23%{%%k4%}      \t\n"
    " vmulpd %%zmm24, %%zmm16, %%zmm15                 \t\n"
    " vfmadd231pd %%zmm25, %%zmm17, %%zmm15            \t\n"
    " vfmadd231pd %%zmm26, %%zmm18, %%zmm15            \t\n"
    " vfmadd231pd %%zmm27, %%zmm19, %%zmm15            \t\n"
    " prefetcht2 0x1200(%[VAL])                        \t\n"
    " prefetcht2 0x1240(%[VAL])                        \t\n"
    " prefetcht2 0x1280(%[VAL])                        \t\n"
    " prefetcht2 0x12c0(%[VAL])                        \t\n"
    " add $0x80, %[COL]                                \t\n"
    " add $0x100, %[VAL]                               \t\n"
    " vmovupd %%zmm4, %%zmm0                           \t\n"
    " vmovupd %%zmm5, %%zmm1                           \t\n"
    " vmovupd %%zmm6, %%zmm2                           \t\n"
    " vmovupd %%zmm7, %%zmm3                           \t\n"
    " vmovupd %%zmm28, %%zmm24                         \t\n"
    " vmovupd %%zmm29, %%zmm25                         \t\n"
    " vmovupd %%zmm30, %%zmm26                         \t\n"
    " vmovupd %%zmm31, %%zmm27                         \t\n"
    " vmovupd %%zmm20, %%zmm16                         \t\n"
    " vmovupd %%zmm21, %%zmm17                         \t\n"
    " vmovupd %%zmm22, %%zmm18                         \t\n"
    " vmovupd %%zmm23, %%zmm19                         \t\n"

    " sub $0x1, %[I]                                   \t\n"
    " jnz loop_spmv                                    \t\n"

    " movl $0x33, %[I]                                 \t\n"
    " kmovw %[I], %%k1                                 \t\n"
    " knotw %%k1, %%k2                                 \t\n"
    " vinsertf64x4 $0x0, %%ymm8, %%zmm12, %%zmm4       \t\n"
    " valignq $0x4, %%zmm8, %%zmm12, %%zmm12           \t\n"
    " vaddpd %%zmm12, %%zmm4, %%zmm4                   \t\n"
    " vinsertf64x4 $0x0, %%ymm9, %%zmm13, %%zmm5       \t\n"
    " valignq $0x4, %%zmm9, %%zmm13, %%zmm13           \t\n"
    " vaddpd %%zmm13, %%zmm5, %%zmm5                   \t\n"
    " vinsertf64x4 $0x0, %%ymm10, %%zmm14, %%zmm6      \t\n"
    " valignq $0x4, %%zmm10, %%zmm14, %%zmm14          \t\n"
    " vaddpd %%zmm14, %%zmm6, %%zmm6                   \t\n"
    " vinsertf64x4 $0x0, %%ymm11, %%zmm15, %%zmm7      \t\n"
    " valignq $0x4, %%zmm11, %%zmm15, %%zmm15          \t\n"
    " vaddpd %%zmm15, %%zmm7, %%zmm7                   \t\n"
    " vmovupd %%zmm6, %%zmm8                           \t\n"
    " vpermpd $0x4e, %%zmm4, %%zmm6%{%%k1%}            \t\n"
    " vpermpd $0x4e, %%zmm8, %%zmm4%{%%k2%}            \t\n"
    " vaddpd %%zmm6, %%zmm4, %%zmm8                    \t\n"
    " vmovupd %%zmm7, %%zmm4                           \t\n"
    " vpermpd $0x4e, %%zmm5, %%zmm7%{%%k1%}            \t\n"
    " vpermpd $0x4e, %%zmm4, %%zmm5%{%%k2%}            \t\n"
    " vaddpd %%zmm7, %%zmm5, %%zmm4                    \t\n"
    " vshufpd $0xaa, %%zmm4, %%zmm8, %%zmm5            \t\n"
    " vshufpd $0x55, %%zmm4, %%zmm8, %%zmm8            \t\n"
    " vaddpd %%zmm8, %%zmm5, %%zmm5                    \t\n"
    " vmovupd %%zmm5, (%[Y])                           \t\n"

    " add $0x40, %[Y]                                  \t\n"
    " movl $0x8, %[I]                                  \t\n"
    " sub $0x1, %[NROW]                                \t\n"
    " jnz loop_spmv                                    \t\n"
    : [NROW]"+r"(nrow), [COL]"+r"(col), [X]"+r"(x), [VAL]"+r"(val), [TMP]"+r"(tmp), [Y]"+r"(y), [I]"+r"(i)
    :
    : "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5", "zmm6", "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13", "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19", "zmm20", "zmm21", "zmm22", "zmm23", "zmm24", "zmm25", "zmm26", "zmm27", "zmm28", "zmm29", "zmm30", "zmm31", "k1", "k2", "k3", "k4"
    );

    return 0;
}

